{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import datetime\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "    \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99998997  anarchists advocate social relations based upon voluntary assoc\n",
      "1003 ritarian political structures and coercive economic institutions\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1003\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[-64:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 + 1 + 1 + 1 # [a-z] + ' ' + GO + END + PAD\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    elif char == 'G':\n",
    "        return 27\n",
    "    elif char == 'E':\n",
    "        return 28\n",
    "    elif char == 'P':\n",
    "        return 29\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "    \n",
    "def id2char(dictid):\n",
    "    if dictid > 0 and dictid < 27: # id(z) = 26\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    elif dictid == 27:\n",
    "        return 'G'\n",
    "    elif dictid == 28:\n",
    "        return 'E'\n",
    "    elif dictid == 29:\n",
    "        return 'P'\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution \n",
    "    assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's obtain targets for batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reversed_batches(seqs_):\n",
    "    if type(seqs_) is str:\n",
    "        seqs = [seqs_]\n",
    "    else:\n",
    "        seqs = seqs_\n",
    "    reversed_ = []\n",
    "    for seq in seqs:\n",
    "        r = ''\n",
    "        seq_list = seq.split(' ')\n",
    "        for word in seq_list:\n",
    "            if len(word) > 0 and word[0] != 'E':\n",
    "                r += word[::-1] + ' '\n",
    "            else:\n",
    "                r += word + ' '\n",
    "        reversed_.append(r[:-1])\n",
    "    if type(seqs_) is str:\n",
    "        return reversed_[0]\n",
    "    return reversed_\n",
    "\n",
    "#batches = train_batches.next()\n",
    "#print (batches2string(batches))\n",
    "#print (reversed_batches(batches2string(batches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't start it everytime! It is so long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reversed_train_text = reversed_batches(train_text)\n",
    "reversed_valid_text = reversed_batches(valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "valid_batch_size = 1\n",
    "num_unrollings = 20\n",
    "\n",
    "class SeqBatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        appr_segment = self._text_size // batch_size\n",
    "        self._cursor= np.zeros(self._batch_size, dtype='int')\n",
    "        for b in range(1,self._batch_size):\n",
    "            space = self._text[self._cursor[b-1]:(self._cursor[b-1] + appr_segment)].rfind(' ')\n",
    "            self._cursor[b] = self._cursor[b-1] + space\n",
    "        self._last_batch = self._next_batch(step=0,\n",
    "                                            batches_last_sym=self._num_unrollings * np.ones(self._batch_size))\n",
    "    \n",
    "    def _next_batch(self, step, batches_last_sym):\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), \n",
    "                         dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            # первый символ - находим конец предложения\n",
    "            if step == 0:\n",
    "                # если предпоследний символ - пробел, то все ок (последний всегда E после пробела)\n",
    "                if (char2id(self._text[(self._cursor[b] + self._num_unrollings - 2) % self._text_size]) == 0):\n",
    "                    batch[b, char2id(self._text[self._cursor[b]])] = 1\n",
    "                    self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "                else:\n",
    "                    if self._cursor[b] + self._num_unrollings < self._text_size:\n",
    "                        seq = self._text[self._cursor[b]:self._cursor[b] + self._num_unrollings]\n",
    "                    else:\n",
    "                        seq = (self._text[self._cursor[b]:]\n",
    "                               + self._text[:(self._cursor[b] + self._num_unrollings)%self._text_size])\n",
    "                    # последнее вхождение пробела\n",
    "                    if (seq.rfind(' ') == self._num_unrollings - 1):\n",
    "                        batches_last_sym[b] = seq[:seq.rfind(' ')].rfind(' ')\n",
    "                    else:\n",
    "                        batches_last_sym[b] = seq.rfind(' ')  \n",
    "                    batch[b, char2id(self._text[self._cursor[b]])] = 1\n",
    "                    self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "            # конец предложения, но не последний символ\n",
    "            elif step == batches_last_sym[b] + 1:\n",
    "                batch[b, 28] = 1 # E - end\n",
    "            elif step > batches_last_sym[b] + 1:\n",
    "                batch[b, 29] = 1 # P - padding\n",
    "            # символ в середине предложения, последний символ, если он конец предложения\n",
    "            else:\n",
    "                batch[b, char2id(self._text[self._cursor[b]])] = 1\n",
    "                self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        batches = [self._last_batch]\n",
    "        batches_last_sym = (self._num_unrollings - 2) * np.ones(self._batch_size)\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch(step, batches_last_sym))\n",
    "        last_batch = np.zeros((self._batch_size, vocabulary_size), dtype='int')\n",
    "        for b in range(self._batch_size):\n",
    "            last_batch[b, char2id(self._text[self._cursor[b]-1])] = 1\n",
    "        self._last_batch = last_batch\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = SeqBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = SeqBatchGenerator(valid_text, valid_batch_size, num_unrollings)\n",
    "\n",
    "#print(batches2string(train_batches.next()))\n",
    "#print(batches2string(train_batches.next()))\n",
    "#print(batches2string(valid_batches.next()))\n",
    "#print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reversed_train_batches = SeqBatchGenerator(reversed_train_text, batch_size, num_unrollings)\n",
    "reversed_valid_batches = SeqBatchGenerator(reversed_valid_text, valid_batch_size, num_unrollings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batches_onehot2ids(batches):\n",
    "    num_batches = len(batches)\n",
    "    batch_size = batches[0].shape[0]\n",
    "    batches_ids = []\n",
    "    for batch in batches:\n",
    "        #batch_ids = np.zeros((batch_size),dtype='int')\n",
    "        batch_ids = np.array([np.argmax(batch[i,:]) for i in range(batch_size)])\n",
    "        batches_ids.append(batch_ids)\n",
    "    return batches_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 64) (32, 64) (20, 32, 128)\n",
      "Graph is ready!\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Parameters:\n",
    "    encod_ifcox_forward = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    encod_ifcom_forward = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    encod_ifcob_forward = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    encod_saved_output_forward = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    encod_saved_state_forward = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    encod_ifcox_backward = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    encod_ifcom_backward = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    encod_ifcob_backward = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    encod_saved_output_backward = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    encod_saved_state_backward = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    encod_ifcox_up = tf.Variable(tf.truncated_normal([2 * num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    encod_ifcom_up = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    encod_ifcob_up = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    encod_saved_output_up = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    encod_saved_state_up = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    decod_ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    decod_ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    decod_ifcob = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    decod_saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    decod_saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the encoder and decoder cells computation.\n",
    "    def encod_lstm_cell_forward(i, o, state):\n",
    "        summ = tf.matmul(i, encod_ifcox_forward) + tf.matmul(o, encod_ifcom_forward) + encod_ifcob_forward\n",
    "        state = (tf.sigmoid(summ[:,num_nodes:2*num_nodes]) * state +\n",
    "                 tf.sigmoid(summ[:,:num_nodes]) * tf.tanh(summ[:,2*num_nodes:3*num_nodes]))\n",
    "        return tf.sigmoid(summ[:,3*num_nodes:]) * tf.tanh(state), state\n",
    "    \n",
    "    def encod_lstm_cell_backward(i, o, state):\n",
    "        summ = tf.matmul(i, encod_ifcox_backward) + tf.matmul(o, encod_ifcom_backward) + encod_ifcob_backward\n",
    "        state = (tf.sigmoid(summ[:,num_nodes:2*num_nodes]) * state +\n",
    "                 tf.sigmoid(summ[:,:num_nodes]) * tf.tanh(summ[:,2*num_nodes:3*num_nodes]))\n",
    "        return tf.sigmoid(summ[:,3*num_nodes:]) * tf.tanh(state), state\n",
    "    \n",
    "    def encod_lstm_cell_up(i, o, state):\n",
    "        summ = tf.matmul(i, encod_ifcox_up) + tf.matmul(o, encod_ifcom_up) + encod_ifcob_up\n",
    "        state = (tf.sigmoid(summ[:,num_nodes:2*num_nodes]) * state +\n",
    "                 tf.sigmoid(summ[:,:num_nodes]) * tf.tanh(summ[:,2*num_nodes:3*num_nodes]))\n",
    "        return tf.sigmoid(summ[:,3*num_nodes:]) * tf.tanh(state), state\n",
    "    \n",
    "    def decod_lstm_cell(i, o, state):\n",
    "        summ = tf.matmul(i, decod_ifcox) + tf.matmul(o, decod_ifcom) + decod_ifcob\n",
    "        state = (tf.sigmoid(summ[:,num_nodes:2*num_nodes]) * state +\n",
    "                 tf.sigmoid(summ[:,:num_nodes]) * tf.tanh(summ[:,2*num_nodes:3*num_nodes]))\n",
    "        return tf.sigmoid(summ[:,3*num_nodes:]) * tf.tanh(state), state\n",
    "    \n",
    "    # Input data.\n",
    "    encod_train_data = list()\n",
    "    decod_train_data = list()\n",
    "    train_labels = list()\n",
    "    \n",
    "    for _ in range(num_unrollings + 1):\n",
    "        #encod_train_data.insert(0, tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "        encod_train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "        decod_train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "        train_labels.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    \n",
    "    encod_train_inputs_forward = encod_train_data[:num_unrollings]\n",
    "    encod_train_inputs_backward = encod_train_data[:num_unrollings][::-1]\n",
    "    decod_train_inputs = decod_train_data[:num_unrollings]\n",
    "    decod_train_labels = train_labels[:num_unrollings]\n",
    "\n",
    "    encod_states_forward = list()\n",
    "    encod_states_backward = list()\n",
    "    decod_outputs = list()\n",
    "    G_input = tf.Variable(27 * tf.ones([batch_size, vocabulary_size], dtype='float32'), trainable=False)\n",
    "    \n",
    "    encod_state_forward = encod_saved_state_forward\n",
    "    encod_output_forward = encod_saved_output_forward\n",
    "    \n",
    "    for i in encod_train_inputs_forward:\n",
    "        encod_output_forward, encod_state_forward = encod_lstm_cell_forward(i, encod_output_forward, encod_state_forward)\n",
    "        encod_states_forward.append(encod_state_forward)\n",
    "        \n",
    "    encod_state_backward = encod_saved_state_backward\n",
    "    encod_output_backward = encod_saved_output_backward\n",
    "    \n",
    "    for i in encod_train_inputs_backward:\n",
    "        encod_output_backward, encod_state_backward = encod_lstm_cell_backward(i, encod_output_backward, encod_state_backward)\n",
    "        encod_states_backward.append(encod_state_backward)\n",
    "    \n",
    "    encod_train_inputs_up = tf.concat([encod_states_forward, encod_states_backward], axis=2)\n",
    "    encod_state_up = encod_saved_state_up\n",
    "    encod_output_up = encod_saved_output_up\n",
    "    print (encod_states_forward[0].shape, encod_states_backward[0].shape, encod_train_inputs_up.shape)\n",
    "    \n",
    "    for i in range(encod_train_inputs_up.shape[0]):\n",
    "        encod_output_up, encod_state_up = encod_lstm_cell_up(encod_train_inputs_up[i,:,:], encod_output_up, encod_state_up)\n",
    "    \n",
    "    decod_state =  decod_saved_state\n",
    "    decod_output = decod_saved_output\n",
    "    decod_output, decod_state = decod_lstm_cell(G_input, decod_output, encod_state_up)\n",
    "    decod_outputs.append(decod_output)\n",
    "    \n",
    "    for i in decod_train_inputs:\n",
    "        decod_output, decod_state = decod_lstm_cell(i, decod_output, encod_state_up)\n",
    "        decod_outputs.append(decod_output)\n",
    "    \n",
    "    with tf.control_dependencies([encod_saved_output_forward.assign(encod_output_forward),\n",
    "                                  encod_saved_output_backward.assign(encod_output_backward),\n",
    "                                  encod_saved_output_up.assign(encod_output_up),\n",
    "                                  decod_saved_output.assign(decod_output),\n",
    "                                  encod_saved_state_forward.assign(encod_state_forward),\n",
    "                                  encod_saved_state_backward.assign(encod_state_backward),\n",
    "                                  encod_saved_state_up.assign(encod_state_up),\n",
    "                                  decod_saved_state.assign(decod_state)]):\n",
    "        decod_logits = tf.nn.xw_plus_b(tf.concat(decod_outputs[:num_unrollings], 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(decod_train_labels, 0), logits=decod_logits)) + 0.0001 * tf.nn.l2_loss(w)\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10., global_step, 1000, 0.9, staircase=False)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(decod_logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    encod_sample_data = list()\n",
    "    decod_sample_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        encod_sample_data.append(tf.placeholder(tf.float32, shape=[valid_batch_size,vocabulary_size]))\n",
    "        decod_sample_data.append(tf.placeholder(tf.float32, shape=[valid_batch_size,vocabulary_size]))\n",
    "    \n",
    "    encod_sample_inputs_forward = encod_sample_data[:num_unrollings]\n",
    "    encod_sample_inputs_backward = encod_sample_data[:num_unrollings][::-1]\n",
    "    decod_sample_inputs = decod_sample_data[:num_unrollings]\n",
    "    \n",
    "    encod_saved_sample_output_forward = tf.Variable(tf.zeros([valid_batch_size, num_nodes]))\n",
    "    encod_saved_sample_state_forward = tf.Variable(tf.zeros([valid_batch_size, num_nodes]))\n",
    "    encod_saved_sample_output_backward = tf.Variable(tf.zeros([valid_batch_size, num_nodes]))\n",
    "    encod_saved_sample_state_backward = tf.Variable(tf.zeros([valid_batch_size, num_nodes]))\n",
    "    encod_saved_sample_output_up = tf.Variable(tf.zeros([valid_batch_size, num_nodes]))\n",
    "    encod_saved_sample_state_up = tf.Variable(tf.zeros([valid_batch_size, num_nodes]))\n",
    "    decod_saved_sample_output = tf.Variable(tf.zeros([valid_batch_size, num_nodes]))\n",
    "    decod_saved_sample_state = tf.Variable(tf.zeros([valid_batch_size, num_nodes]))\n",
    "    \n",
    "    reset_sample_state = tf.group(\n",
    "        encod_saved_sample_output_forward.assign(tf.zeros([valid_batch_size, num_nodes])),\n",
    "        encod_saved_sample_state_forward.assign(tf.zeros([valid_batch_size, num_nodes])),\n",
    "        encod_saved_sample_output_backward.assign(tf.zeros([valid_batch_size, num_nodes])),\n",
    "        encod_saved_sample_state_backward.assign(tf.zeros([valid_batch_size, num_nodes])),\n",
    "        encod_saved_sample_output_up.assign(tf.zeros([valid_batch_size, num_nodes])),\n",
    "        encod_saved_sample_state_up.assign(tf.zeros([valid_batch_size, num_nodes])),\n",
    "        decod_saved_sample_output.assign(tf.zeros([valid_batch_size, num_nodes])),\n",
    "        decod_saved_sample_state.assign(tf.zeros([valid_batch_size, num_nodes])))\n",
    "\n",
    "    G_input = tf.Variable(27 * tf.ones([valid_batch_size, vocabulary_size], dtype='float32'), trainable=False)\n",
    "    encod_sample_states_forward = list()\n",
    "    encod_sample_states_backward = list()\n",
    "    decod_sample_outputs = list()\n",
    "    encod_sample_state_forward = encod_saved_sample_state_forward\n",
    "    encod_sample_output_forward = encod_saved_sample_output_forward\n",
    "    encod_sample_state_backward = encod_saved_sample_state_backward\n",
    "    encod_sample_output_backward = encod_saved_sample_output_backward\n",
    "    encod_sample_state_up = encod_saved_sample_state_up\n",
    "    encod_sample_output_up = encod_saved_sample_output_up\n",
    "    decod_sample_state = decod_saved_sample_state\n",
    "    decod_sample_output = decod_saved_sample_output\n",
    "    \n",
    "    for i in encod_sample_inputs_forward:\n",
    "        encod_sample_output_forward, encod_sample_state_forward = encod_lstm_cell_forward(i,\n",
    "                                                                                          encod_sample_output_forward, \n",
    "                                                                                          encod_sample_state_forward)\n",
    "        encod_sample_states_forward.append(encod_sample_state_forward)\n",
    "        \n",
    "    for i in encod_sample_inputs_backward:\n",
    "        encod_sample_output_backward, encod_sample_state_backward = encod_lstm_cell_backward(i, \n",
    "                                                                                             encod_sample_output_backward, \n",
    "                                                                                             encod_sample_state_backward)\n",
    "        encod_sample_states_backward.append(encod_sample_state_backward)\n",
    "    \n",
    "    encod_sample_inputs_up = tf.concat([encod_sample_states_forward, encod_sample_states_backward], axis=2)\n",
    "    for i in range(encod_sample_inputs_up.shape[0]):\n",
    "        encod_sample_output_up, encod_sample_state_up = encod_lstm_cell_up(encod_sample_inputs_up[i,:,:], \n",
    "                                                                           encod_sample_output_up, \n",
    "                                                                           encod_sample_state_up)\n",
    "        \n",
    "    decod_sample_output, decod_sample_state = decod_lstm_cell(G_input, \n",
    "                                                                  decod_sample_output,\n",
    "                                                                  encod_sample_state_up)\n",
    "    decod_sample_outputs.append(decod_sample_output)\n",
    "\n",
    "    for i in range(1, num_unrollings):\n",
    "        decod_sample_input_ = tf.nn.softmax(tf.nn.xw_plus_b(decod_sample_outputs[i-1], w, b))\n",
    "        decod_sample_output, decod_sample_state = decod_lstm_cell(decod_sample_input_, \n",
    "                                                                  decod_sample_output,\n",
    "                                                                  encod_sample_state_up)\n",
    "        decod_sample_outputs.append(decod_sample_output)\n",
    "\n",
    "    with tf.control_dependencies([encod_saved_sample_output_forward.assign(encod_sample_output_forward),\n",
    "                                  encod_saved_sample_state_forward.assign(encod_sample_state_forward),\n",
    "                                  encod_saved_sample_output_backward.assign(encod_sample_output_backward),\n",
    "                                  encod_saved_sample_state_backward.assign(encod_sample_state_backward),\n",
    "                                  encod_saved_sample_output_up.assign(encod_sample_output_up),\n",
    "                                  encod_saved_sample_state_up.assign(encod_sample_state_up),\n",
    "                                  decod_saved_sample_output.assign(decod_sample_output),\n",
    "                                  decod_saved_sample_state.assign(decod_sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(tf.concat(decod_sample_outputs[:num_unrollings],0), w, b))\n",
    "\n",
    "    print('Graph is ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Predicted train text:  gogogvgogogvgogogvg faffvfbgogogvgbgbgb gogogogogogvgbgbgog gogbgbgbgogvgogogbg gfgvgogvgbgogbgogbg gogogogvgvgogbgbgbg gogogbgbgogogogogbg gogogvgvgogogogbgbg gvgogogbgogbgbgbgbg ogvhogbgohbgbgogbgb gvgogbgogvgvgvgvgvg gbgbgbgvgvgogogogbg gogogogbgvgogbgogog glgbgbgbgvgbgbgbgbg gfgbgbgbgbgbgogogbg gbgbgbgbgbgbgbgogog oevgogbhogbhbgbgohb gvgogvgogogvgvgvgvg gvgvgvgogogogbgogog gugogogogvgogogogbg gvgogogogogogogogvg gvgogogvgogogogogog gbgogbgogbgogbgbgbg gvgogogogogogogogbg gbgvgogvgogogbgbgbg grgogvgvgvgbgbgbgbg gvgvgogogogbgvgogog gbgbgogogogogogogog gvgvgogogogogogbgbg gvgbgogvgbgbgbgbgbg gvgvgbgbgvgogvgogbg flgbgogvgbgogbgbgbg\n",
      "Average loss at step 0: 3.402738 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.99\n",
      "Valid text:      describe any act EPP\n",
      "Reversed text:   ebircsed yna tca EPP\n",
      "Predicted text:                     \n",
      "Valid text:      does not imply EPPPP\n",
      "Reversed text:   seod ton ylpmi EPPPP\n",
      "Predicted text:                     \n",
      "Valid text:      defined anarchists E\n",
      "Reversed text:   denifed stsihcrana E\n",
      "Predicted text:                     \n",
      "Valid text:      authoritarian EPPPPP\n",
      "Reversed text:   nairatirohtua EPPPPP\n",
      "Predicted text:                     \n",
      "Valid text:      philosophy is the EP\n",
      "Reversed text:   yhposolihp si eht EP\n",
      "Predicted text:                     \n",
      "Valid text:      first used against E\n",
      "Reversed text:   tsrif desu tsniaga E\n",
      "Predicted text:                     \n",
      "Valid text:      what this means EPPP\n",
      "Reversed text:   tahw siht snaem EPPP\n",
      "Predicted text:                     \n",
      "Valid text:      french revolution EP\n",
      "Reversed text:   hcnerf noitulover EP\n",
      "Predicted text:                     \n",
      "Valid text:      particularly the EPP\n",
      "Reversed text:   ylralucitrap eht EPP\n",
      "Predicted text:                     \n",
      "Valid text:      of society it has EP\n",
      "Reversed text:   fo yteicos ti sah EP\n",
      "Predicted text:                     \n",
      "Valid text:      authoritarian EPPPPP\n",
      "Reversed text:   nairatirohtua EPPPPP\n",
      "Predicted text:                     \n",
      "Validation set perplexity: 21.21504\n",
      "\n",
      "Predicted train text:  eis esr esemesei eP eehes he tae  EPPPP eha  r entEetlaee E eever  ei eht EPPPP eeha aer E nal a EP ei eht EPPPPPPPPPPP eanirala etib a a E sseroiir EPPPPPPPPP eetetel  earter EPP sioiso soc i  eo EP ei eesloo a eo  el  eetafsr e er  r EPP anPPPPPPPPPPPPPPPPP een n eno enin EPPP eanirar e  a eehea  er nwl s Eet oei eP eeteeer esel e EPPP eetaes l Eohso EPPP ehres g r Eht EPPPP eliaes elatoa EPPPP entnis ew i eol el  eetalarr eht Eiw EP elnl rr senin r EPP eetionre e el eht E eessi   et a EPPPPP eht eetsloeec eneh  ehot r r  plEha EPP ei tev l ea el eee  eoeoe lnoovea  EPPP ee eetev o  el ePPP erez ooof Ehgie EPP eilailavaa t s EPPP\n",
      "Average loss at step 1000: 1.780084 learning rate: 9.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Valid text:      regarded as EPPPPPPP\n",
      "Reversed text:   dedrager sa EPPPPPPP\n",
      "Predicted text:  erataepee  sihaaa E\n",
      "Valid text:      political EPPPPPPPPP\n",
      "Reversed text:   lacitilop EPPPPPPPPP\n",
      "Predicted text:  siitiicioc ri EPPPP\n",
      "Valid text:      term of abuse EPPPPP\n",
      "Reversed text:   mret fo esuba EPPPPP\n",
      "Predicted text:  erotaeper    ti tio\n",
      "Valid text:      interpretations of E\n",
      "Reversed text:   snoitaterpretni fo E\n",
      "Predicted text:  siitiir er  ri  ei \n",
      "Valid text:      culottes of the EPPP\n",
      "Reversed text:   settoluc fo eht EPPP\n",
      "Predicted text:  srotsopsor si E o s\n",
      "Valid text:      institutions EPPPPPP\n",
      "Reversed text:   snoitutitsni EPPPPPP\n",
      "Predicted text:  siiitiiiiiis EPPPPP\n",
      "Valid text:      the organization EPP\n",
      "Reversed text:   eht noitazinagro EPP\n",
      "Predicted text:  erotser rro rr  rol\n",
      "Valid text:      a harmonious anti EP\n",
      "Reversed text:   a suoinomrah itna EP\n",
      "Predicted text:  drotaraa  Eralaap a\n",
      "Valid text:      the greek without EP\n",
      "Reversed text:   eht keerg tuohtiw EP\n",
      "Predicted text:  eriteeer r er EPPPP\n",
      "Valid text:      coercive economic EP\n",
      "Reversed text:   evicreoc cimonoce EP\n",
      "Predicted text:  srotnir rr or EPPPP\n",
      "Valid text:      and should be EPPPPP\n",
      "Reversed text:   dna dluohs eb EPPPPP\n",
      "Predicted text:  drotaopor Eratsrp s\n",
      "Validation set perplexity: 26.12174\n",
      "\n",
      "Predicted train text:  oo ooitss p o Et EP esemef erew EPPPPPP sossoss stlop o EPP elpaeee elt EPPPPPP ef ehgie Ehif EPPPP seiirir siiir EPPPP ssosooosos saoomsPP ro d rom Eawr bt EP nnonsots oc EeEPPPP soretht eerars EPPP sciwirb Eeel  r EPP tnemerevee Et EPPPP senalgnal ssina  EP eerht eerht EPPPPPP snidudnsnoc nbni n  do dnem od forf EPP rof eht sreif ehit  tneteserp eo eht EP nnircsnr eimeevirpr srtiih sni Eis EPPP lellr Ehbts Eo EPPP sie srhhih  EPPPPPP seccisc oc o EPPPPP n nmnno eno Ena EPP nea eedo   e eneno  e erim  Erph r  EPP n n aa  n a na nno  seere serepsr sor   secueresp shpose eo ttt tehothem Eo EPP rnemrrovor Eet EPPP settorerettrr o  EP\n",
      "Average loss at step 2000: 1.479222 learning rate: 8.099999\n",
      "Minibatch perplexity: 3.52\n",
      "Valid text:      differing EPPPPPPPPP\n",
      "Reversed text:   gnireffid EPPPPPPPPP\n",
      "Predicted text:  diitin ff teriEed n\n",
      "Valid text:      and the sans EPPPPPP\n",
      "Reversed text:   dna eht snas EPPPPPP\n",
      "Predicted text:  snns  s  etat eet E\n",
      "Valid text:      authoritarian EPPPPP\n",
      "Reversed text:   nairatirohtua EPPPPP\n",
      "Predicted text:  nairataar   EPPPara\n",
      "Valid text:      means to destroy EPP\n",
      "Reversed text:   snaem ot yortsed EPP\n",
      "Predicted text:  saooa  a   aa      \n",
      "Valid text:      anomie but rather EP\n",
      "Reversed text:   eimona tub rehtar EP\n",
      "Predicted text:  eei er ereta  Erot \n",
      "Valid text:      is derived from EPPP\n",
      "Reversed text:   si devired morf EPPP\n",
      "Predicted text:  si ri  fred EPPPPPP\n",
      "Valid text:      structures and EPPPP\n",
      "Reversed text:   serutcurts dna EPPPP\n",
      "Predicted text:  srrtturra  EPPPPPPP\n",
      "Valid text:      are unnecessary EPPP\n",
      "Reversed text:   era yrassecennu EPPP\n",
      "Predicted text:  seriereereeeer eeep\n",
      "Valid text:      class radicals EPPPP\n",
      "Reversed text:   ssalc slacidar EPPPP\n",
      "Predicted text:  saaisalss lala sala\n",
      "Valid text:      refers to related EP\n",
      "Reversed text:   srefer ot detaler EP\n",
      "Predicted text:  sereeree e  ser s s\n",
      "Valid text:      still used in a EPPP\n",
      "Reversed text:   llits desu ni a EPPP\n",
      "Predicted text:  slili  slll   lal  \n",
      "Validation set perplexity: 23.95750\n",
      "\n",
      "Predicted train text:  saar a ttatotaPPPPP eerht enahaenaegeee eht eaeltaaelaPPPPP sa  siorrab sPPPPPP llaaccala naiaPPPPP gnaaaab sanhgtgngPP owt ott ooow ohrht  oo sht sioit vif sP ettotitnatnnac eeeP tnelntaltttnanntPPP ni ai aiii  ab nPPP laamoatl llaollaaPa nomog nnn tnatgan n saeelasataalael so  ruuuum sirruusium u allalalaatlalaPPPPP ytitummoc ehaadt eP sht sioitoere ihsPP seigalmoc saalaaaaa slilitataaiistiltta ecalaab aedallaaeaa aaitilitailiniiiiti seitosuoc stis sihs ssocc taelcoc ttiw  htitihbasiih rtitPP neeb neof nnee  nPP rev aiedaw l   yPPP dna aanadanaal da a ttna a aaanntatna a seiiaeb sii svaiiaP nninidem nneni ePPP hsuoht shoh eho ePP\n",
      "Average loss at step 3000: 1.342455 learning rate: 7.289999\n",
      "Minibatch perplexity: 5.78\n",
      "Valid text:      that used violent EP\n",
      "Reversed text:   taht desu tneloiv EP\n",
      "Predicted text:  tsem ten e  EPPPPPP\n",
      "Valid text:      chaos nihilism or EP\n",
      "Reversed text:   soahc msilihin ro EP\n",
      "Predicted text:  semo sih EPPPPPPPPP\n",
      "Valid text:      the word anarchism E\n",
      "Reversed text:   eht drow msihcrana E\n",
      "Predicted text:  eho sror s sirric E\n",
      "Valid text:      political EPPPPPPPPP\n",
      "Reversed text:   lacitilop EPPPPPPPPP\n",
      "Predicted text:  ycilop EPPPPPPPPPPP\n",
      "Valid text:      belief that rulers E\n",
      "Reversed text:   feileb taht srelur E\n",
      "Predicted text:  sel e  seviep Ei  e\n",
      "Valid text:      early working EPPPPP\n",
      "Reversed text:   ylrae gnikrow EPPPPP\n",
      "Predicted text:  yrom Erom EPPPPPPPP\n",
      "Valid text:      anarchism also EPPPP\n",
      "Reversed text:   msihcrana osla EPPPP\n",
      "Predicted text:  srrrs s sa EPPPPPPP\n",
      "Valid text:      whilst the term is E\n",
      "Reversed text:   tslihw eht mret si E\n",
      "Predicted text:  stur s  ee   e es E\n",
      "Valid text:      state the word EPPPP\n",
      "Reversed text:   etats eht drow EPPPP\n",
      "Predicted text:  esos eho EPPPPPPPPP\n",
      "Valid text:      also been taken up E\n",
      "Reversed text:   osla neeb nekat pu E\n",
      "Predicted text:  eno snooe ee e  eno\n",
      "Valid text:      society in place EPP\n",
      "Reversed text:   yteicos ni ecalp EPP\n",
      "Predicted text:  ccito se E ei EPPPP\n",
      "Validation set perplexity: 60.88561\n",
      "\n",
      "Predicted train text:  eno ro enim EPPPPPP esenapap EPPPPPPPPP ssecniff ehucccicEP eht noitarommm oc E eetamiddiono owt EP detsts da  dnnuw EP dna s naofsa  na EP eht llrow EPPPPPPPP sraey nht eeh  EPPP rednaw dorf eht EPP yop stoc tna ecouc  noitcudorp ecihw EP dna a nganidq ni EP yccsm yimub ymaut E eniaamam eneww ehtE aaaraac a araop EPP erehpsoniifni EPPPP yurrg yetaugnoc ye  lacccirrm EPPPPPPPP ecac daht ana aa a  esuaceb fo eht EPPP eht ennn niht a EPP ddanac dea detanunE deraaams yb ri EPPP thgirepmr ttirrp EP eht ycccnyiccucc yP setitsts detited EP srrrehtoon s h ti E nrodot snnoduomsaEP fis thgie or  EPPPP nehw enanttntnac EP fo niatreceEPPPPPPP\n",
      "Average loss at step 4000: 1.042083 learning rate: 6.560999\n",
      "Minibatch perplexity: 2.26\n",
      "Valid text:      authoritarian EPPPPP\n",
      "Reversed text:   nairatirohtua EPPPPP\n",
      "Predicted text:  nairataar ni EPPPPP\n",
      "Valid text:      philosophy is the EP\n",
      "Reversed text:   yhposolihp si eht EP\n",
      "Predicted text:  hhohpopo  hhh eht E\n",
      "Valid text:      first used against E\n",
      "Reversed text:   tsrif desu tsniaga E\n",
      "Predicted text:  tsrit tsorat tsoria\n",
      "Valid text:      what this means EPPP\n",
      "Reversed text:   tahw siht snaem EPPP\n",
      "Predicted text:  taht hiirow EPPPPPP\n",
      "Valid text:      french revolution EP\n",
      "Reversed text:   hcnerf noitulover EP\n",
      "Predicted text:  hgnerf nioeera EPPP\n",
      "Valid text:      particularly the EPP\n",
      "Reversed text:   ylralucitrap eht EPP\n",
      "Predicted text:  ylratiuaia eh eh he\n",
      "Valid text:      of society it has EP\n",
      "Reversed text:   fo yteicos ti sah EP\n",
      "Predicted text:  foosos ta  EPPPPPPP\n",
      "Valid text:      authoritarian EPPPPP\n",
      "Reversed text:   nairatirohtua EPPPPP\n",
      "Predicted text:  nairataao ar EPPPPP\n",
      "Valid text:      archons ruler EPPPPP\n",
      "Reversed text:   snohcra relur EPPPPP\n",
      "Predicted text:  snorar seer EPPPPPP\n",
      "Valid text:      institutions EPPPPPP\n",
      "Reversed text:   snoitutitsni EPPPPPP\n",
      "Predicted text:  snoitidtntna EPPPPP\n",
      "Valid text:      abolished although E\n",
      "Reversed text:   dehsiloba hguohtla E\n",
      "Predicted text:  dehooo o ehtoh eht \n",
      "Validation set perplexity: 12.92066\n",
      "\n",
      "Predicted train text:  gniniitcnif dorf EP etirefaf svat ei EP sterces fo eht EPPP thgie thgie owt EPP eht eea eno ott EPP ela  a laccips EPPP gniniarer ni EPPPPP nacirema tnarceaa E yletalpmoc EPPPPPPP ynam seetseedaehtEP fo hciee  no eeete  esuaceb est yreb EP eht yilur si laua E erom elbitapmoc EPP thgie ehrht namreg  fo dsnnomoc s  fna  dlrroo eiil epee i  eb e elo telolloc e owt orez orez evif  ot ei nanaiiil inia hsihosos tcadtn EPP sedormoc deenunuhcE ybolofhcss ecuf e E stsurer elew EPPPPP atannnnonnoaott EPP siliq seattaap saw  giiabaac tnniatacEP detagrrtetni na EPP eno enin evif EPPPP enin euof et eeves  truppup nnpacp no E neves evo evif EPPP\n",
      "Average loss at step 5000: 0.802858 learning rate: 5.904899\n",
      "Minibatch perplexity: 2.03\n",
      "Valid text:      interpretations of E\n",
      "Reversed text:   snoitaterpretni fo E\n",
      "Predicted text:  snoitalretne fo do \n",
      "Valid text:      culottes of the EPPP\n",
      "Reversed text:   settoluc fo eht EPPP\n",
      "Predicted text:  setaluo fo eht EPPP\n",
      "Valid text:      institutions EPPPPPP\n",
      "Reversed text:   snoitutitsni EPPPPPP\n",
      "Predicted text:  snoitutstsn EPPPPPP\n",
      "Valid text:      the organization EPP\n",
      "Reversed text:   eht noitazinagro EPP\n",
      "Predicted text:  eht noitarigarg EPP\n",
      "Valid text:      a harmonious anti EP\n",
      "Reversed text:   a suoinomrah itna EP\n",
      "Predicted text:  a a ororom air o EP\n",
      "Valid text:      the greek without EP\n",
      "Reversed text:   eht keerg tuohtiw EP\n",
      "Predicted text:  eht teer  tsrgg EPP\n",
      "Valid text:      coercive economic EP\n",
      "Reversed text:   evicreoc cimonoce EP\n",
      "Predicted text:  eciroec rccoroc EPP\n",
      "Valid text:      and should be EPPPPP\n",
      "Reversed text:   dna dluohs eb EPPPPP\n",
      "Predicted text:  dna ylaol yl  EPPPP\n",
      "Valid text:      including the EPPPPP\n",
      "Reversed text:   gnidulcni eht EPPPPP\n",
      "Predicted text:  gnidulcni eht EPPPP\n",
      "Valid text:      social movements EPP\n",
      "Reversed text:   laicos stnemevom EPP\n",
      "Predicted text:  llioom stnomom EPPP\n",
      "Valid text:      pejorative way to EP\n",
      "Reversed text:   evitarojep yaw ot EP\n",
      "Predicted text:  evitaepo  ot ai EPP\n",
      "Validation set perplexity: 12.39796\n",
      "\n",
      "Predicted train text:  nretsee airara EPPP yrotirret yorf EPPP sutrib eno thgie EP dellac EPPPPPPPPPPP eniram staoc na EPP a aaa nna n  EPPPPP snoitansnart htia E slaerts snanac EPPP hguone nef noinen E segahrohs taht EPPP yseeh yraacac EPPPP tcilfnoc tcartnoc E neh  dbam fo eht EP gniwoloof soo  si E lanoitidart laicos  trepme stetiew EPPP noitacilbuc fo eht  orez eno retep EPPP cirtnelohhpel EPPPP ni eih nno thgie EP on eeonnia ennalts  eht yiac yooioc EPP delolla retaert EPP notpaohsoos foo EPP tiiiv tt tsei EPPPP ecit eo eht tss e   rebmeven ott ohgie  tropiia eno ruof EP ni eluuuu ehtnehw E detareetii otni EPP sretcaracc dna d EP namsetsts eneno EPP\n",
      "Average loss at step 6000: 0.685166 learning rate: 5.314409\n",
      "Minibatch perplexity: 1.65\n",
      "Valid text:      means to destroy EPP\n",
      "Reversed text:   snaem ot yortsed EPP\n",
      "Predicted text:  snaem onotsed doiaa\n",
      "Valid text:      anomie but rather EP\n",
      "Reversed text:   eimona tub rehtar EP\n",
      "Predicted text:  emiman stara  ra EP\n",
      "Valid text:      is derived from EPPP\n",
      "Reversed text:   si devired morf EPPP\n",
      "Predicted text:  si d derreed morf E\n",
      "Valid text:      structures and EPPPP\n",
      "Reversed text:   serutcurts dna EPPPP\n",
      "Predicted text:  srrtturssas dna EPP\n",
      "Valid text:      are unnecessary EPPP\n",
      "Reversed text:   era yrassecennu EPPP\n",
      "Predicted text:  era reraneunne sa E\n",
      "Valid text:      class radicals EPPPP\n",
      "Reversed text:   ssalc slacidar EPPPP\n",
      "Predicted text:  ssalc slacirar EPPP\n",
      "Valid text:      refers to related EP\n",
      "Reversed text:   srefer ot detaler EP\n",
      "Predicted text:  sreser deraera dh E\n",
      "Valid text:      still used in a EPPP\n",
      "Reversed text:   llits desu ni a EPPP\n",
      "Predicted text:  llits li  a EPPPPPP\n",
      "Valid text:      anarchy as most EPPP\n",
      "Reversed text:   yhcrana sa tsom EPPP\n",
      "Predicted text:  ybarar sah nom EPPP\n",
      "Valid text:      as a positive EPPPPP\n",
      "Reversed text:   sa a evitisop EPPPPP\n",
      "Predicted text:  ad edoeetisso EPPPP\n",
      "Valid text:      of what are EPPPPPPP\n",
      "Reversed text:   fo tahw era EPPPPPPP\n",
      "Predicted text:  foo eratra EPPPPPPP\n",
      "Validation set perplexity: 9.18545\n",
      "\n",
      "Predicted train text:  eht nala EPPPPPPPPP iiv e eno thgie EPP sanomama snacooc EP erew devor r morf E tfarcra  detiisnd E tnemnrevog detag EP ro rehtee ni EPPPPP raelc troba EPPPPPP derddrum ta evom e  suidualc saw EPPPPP sdraug srassuh EPPP selas erew EPPPPPPP fo ytinummoc EPPPPP nodoohooop hcus EPP wol sciiiicnoc ni E daetsni fo sih EPPP eeht eht sreloorp E neves seves rrrep E gniniigeb ot EPPPPP snaiitsap EPPPPPPPP si noisrrroc EPPPPP sa eht yilium EPPPP srarr dna osla EPPP orez seeorf eht EPP eno neves neves EPP hticm tth na  naEPP setacidni taht eht  ssolocs scdac EPPPP eht tcrtnemroovog E orez orez eerht EPP ekarhetrahhstehtEPP nainggnuhcseeh  EPP\n",
      "Average loss at step 7000: 0.612911 learning rate: 4.782968\n",
      "Minibatch perplexity: 1.56\n",
      "Valid text:      political EPPPPPPPPP\n",
      "Reversed text:   lacitilop EPPPPPPPPP\n",
      "Predicted text:  lacitilop EPPPPPPPP\n",
      "Valid text:      belief that rulers E\n",
      "Reversed text:   feileb taht srelur E\n",
      "Predicted text:  filep  telrel srall\n",
      "Valid text:      early working EPPPPP\n",
      "Reversed text:   ylrae gnikrow EPPPPP\n",
      "Predicted text:  ylrae snilgir o EPP\n",
      "Valid text:      anarchism also EPPPP\n",
      "Reversed text:   msihcrana osla EPPPP\n",
      "Predicted text:  miihsraran osla EPP\n",
      "Valid text:      whilst the term is E\n",
      "Reversed text:   tslihw eht mret si E\n",
      "Predicted text:  tsels  sehti  s s a\n",
      "Valid text:      state the word EPPPP\n",
      "Reversed text:   etats eht drow EPPPP\n",
      "Predicted text:  etats eht dnow EPPP\n",
      "Valid text:      also been taken up E\n",
      "Reversed text:   osla neeb nekat pu E\n",
      "Predicted text:  osna sela eeaa  s a\n",
      "Valid text:      society in place EPP\n",
      "Reversed text:   yteicos ni ecalp EPP\n",
      "Predicted text:  ytinehs e  ecala EP\n",
      "Valid text:      chief king EPPPPPPPP\n",
      "Reversed text:   feihc gnik EPPPPPPPP\n",
      "Predicted text:  fihc gnib EPPPPPPPP\n",
      "Valid text:      anarchism EPPPPPPPPP\n",
      "Reversed text:   msihcrana EPPPPPPPPP\n",
      "Predicted text:  msihsaran EPPPPPPPP\n",
      "Valid text:      there are EPPPPPPPPP\n",
      "Reversed text:   ereht era EPPPPPPPPP\n",
      "Predicted text:  errht era EPPPPPPPP\n",
      "Validation set perplexity: 5.12982\n",
      "\n",
      "Predicted train text:  ass sn  yo rnam ro  eno eis eerft enif  ruof rhrht yaafbd E eltsil etab naht EP noitcetorp ria EPPP gnipmott ddlam dna  seitic stiw a EPPPP rednu oraird EPPPPP msilaannic detab  E rotnac tin tihtoht  ainolatac taa EPPPP laitniiiserp EPPPPP lanretxe eotf fo EP ylliaabror dna EPPP spahceb enieb eht E yllucerrof EPPPPPPP sevitucece ro a EPP tnacitnngin emit EP niac nertao reht  E rehtrr damt denorr  yrosho  retahht a E repyppeen er EPPPPP eht yradnoccs EPPPP hsilgne sresseps EP eeven nrigaaniat  E sderdnub ot og EPPP gnuog tneiat emat E ecnirp eell  eno EP srebmrr y yletoom E ruof navvoa nahvo E desrever evo yaw EP srrocer ot eman ai \n",
      "Average loss at step 8000: 0.544064 learning rate: 4.304671\n",
      "Minibatch perplexity: 1.72\n",
      "Valid text:      what this means EPPP\n",
      "Reversed text:   tahw siht snaem EPPP\n",
      "Predicted text:  tahw sih  snem e EP\n",
      "Valid text:      french revolution EP\n",
      "Reversed text:   hcnerf noitulover EP\n",
      "Predicted text:  hcnerf noitrerov EP\n",
      "Valid text:      particularly the EPP\n",
      "Reversed text:   ylralucitrap eht EPP\n",
      "Predicted text:  yllaucidarp ehhehw \n",
      "Valid text:      of society it has EP\n",
      "Reversed text:   fo yteicos ti sah EP\n",
      "Predicted text:  fo eesioos ti sih E\n",
      "Valid text:      authoritarian EPPPPP\n",
      "Reversed text:   nairatirohtua EPPPPP\n",
      "Predicted text:  nairatirarapa a EPP\n",
      "Valid text:      archons ruler EPPPPP\n",
      "Reversed text:   snohcra relur EPPPPP\n",
      "Predicted text:  snohcr  reluur EPPP\n",
      "Valid text:      institutions EPPPPPP\n",
      "Reversed text:   snoitutitsni EPPPPPP\n",
      "Predicted text:  snoitutitsnis EPPPP\n",
      "Valid text:      abolished although E\n",
      "Reversed text:   dehsiloba hguohtla E\n",
      "Predicted text:  dehsilih gneholoh E\n",
      "Valid text:      diggers of the EPPPP\n",
      "Reversed text:   sreggid fo eht EPPPP\n",
      "Predicted text:  sreggid fo eht EPPP\n",
      "Valid text:      that advocate the EP\n",
      "Reversed text:   taht etacovda eht EP\n",
      "Predicted text:  taht etacodod eht E\n",
      "Valid text:      describe any act EPP\n",
      "Reversed text:   ebircsed yna tca EPP\n",
      "Predicted text:  emilpser tna  ta EP\n",
      "Validation set perplexity: 4.49591\n",
      "\n",
      "Predicted train text:  emit ereht EPPPPPPP eerht t erec errht  etueer eht aivi EPP taht eht EPPPPPPPPP egarttbra EPPPPPPPP dna nnauoohcp si EP sih seof seinammoc  htireep srettaah EP eht hsiwek EPPPPPPP reporp tsssus fo EP thgie owt enin EPPP erodommoc ris ruof  rehtona elpaace EPP dna sht dporrorhoh  tnemele ni ecatan E tnatsetsrp sellaa E trebroh salaiigrro  eht ssaisid sti EPP reppu eoitced fo EP sesu noituuuf ro EP cittilaaala  EPPPPP msirorret dna EPPPP eht cilbuper fo EPP tnemevom fo EPPPPPP eerht enin sis et E srehcnar dna EPPPPP nala dnanocaam dht  suoirav suuorg EPPP sa  stanarg sa EPPP lam elalauo fo eht  gnippam ot eh EPPPP eht ddnrrb dedep  E\n",
      "Average loss at step 9000: 0.502528 learning rate: 3.874204\n",
      "Minibatch perplexity: 1.51\n",
      "Valid text:      the organization EPP\n",
      "Reversed text:   eht noitazinagro EPP\n",
      "Predicted text:  eht noitanignag not\n",
      "Valid text:      a harmonious anti EP\n",
      "Reversed text:   a suoinomrah itna EP\n",
      "Predicted text:  a sinonorra sinom E\n",
      "Valid text:      the greek without EP\n",
      "Reversed text:   eht keerg tuohtiw EP\n",
      "Predicted text:  eht teereg tuowgih \n",
      "Valid text:      coercive economic EP\n",
      "Reversed text:   evicreoc cimonoce EP\n",
      "Predicted text:  evimeroc cimonec EP\n",
      "Valid text:      and should be EPPPPP\n",
      "Reversed text:   dna dluohs eb EPPPPP\n",
      "Predicted text:  dna yluoss el EPPPP\n",
      "Valid text:      including the EPPPPP\n",
      "Reversed text:   gnidulcni eht EPPPPP\n",
      "Predicted text:  gnidulcni eht EPPPP\n",
      "Valid text:      social movements EPP\n",
      "Reversed text:   laicos stnemevom EPP\n",
      "Predicted text:  laicos stnemimom EP\n",
      "Valid text:      pejorative way to EP\n",
      "Reversed text:   evitarojep yaw ot EP\n",
      "Predicted text:  evitaromot ot a ot \n",
      "Valid text:      anarchists use it EP\n",
      "Reversed text:   stsihcrana esu ti EP\n",
      "Predicted text:  stsihcnar ehic th E\n",
      "Valid text:      label by self EPPPPP\n",
      "Reversed text:   lebal yb fles EPPPPP\n",
      "Predicted text:  lebal yb yless EPPP\n",
      "Valid text:      regarded as EPPPPPPP\n",
      "Reversed text:   dedrager sa EPPPPPPP\n",
      "Predicted text:  dedrager sa EPPPPPP\n",
      "Validation set perplexity: 4.02071\n",
      "\n",
      "Predicted train text:  eht elorofo eht EPP naa sretaahc EPPPPP ylevitca EPPPPPPPPP egaugnal fo atihc E dellik yb eht EPPPP fo eht retnuh eht E ynno snikoob htiw E hcahgooopp nehap EP tnemele n noiteeuq  dliw esohh esohc EP citsilarttar eeidaE detinu dodgnik dna  yllacitnevidot EPPP emos fo eht EPPPPPP desopmoc fo reh EPP trecnoc EPPPPPPPPPP s erasoitcudort i E emas taal eman EPPP orez orez trez tse  no dhgdednatta eht  nosrep naht elamef  leehs saht saw a EP eht ecapp fo EPPPPP eht noisrev fo EPPP ssrif ssnngnoc si E hakat snar  ybor EP erof esrec   ssal   mutamitlu EPPPPPPPP srellr evr evess EP sraey gnitaats si E tfarciac tnuoc EPPP evu nnnogaab  ni EP\n",
      "Average loss at step 10000: 0.465502 learning rate: 3.486784\n",
      "Minibatch perplexity: 1.42\n",
      "Valid text:      structures and EPPPP\n",
      "Reversed text:   serutcurts dna EPPPP\n",
      "Predicted text:  serutcurts dna EPPP\n",
      "Valid text:      are unnecessary EPPP\n",
      "Reversed text:   era yrassecennu EPPP\n",
      "Predicted text:  era yrisnenecnune E\n",
      "Valid text:      class radicals EPPPP\n",
      "Reversed text:   ssalc slacidar EPPPP\n",
      "Predicted text:  ssalc slacidar EPPP\n",
      "Valid text:      refers to related EP\n",
      "Reversed text:   srefer ot detaler EP\n",
      "Predicted text:  srefer oo detaare E\n",
      "Valid text:      still used in a EPPP\n",
      "Reversed text:   llits desu ni a EPPP\n",
      "Predicted text:  llits tah  a na EPP\n",
      "Valid text:      anarchy as most EPPP\n",
      "Reversed text:   yhcrana sa tsom EPPP\n",
      "Predicted text:  yhprara si sono EPP\n",
      "Valid text:      as a positive EPPPPP\n",
      "Reversed text:   sa a evitisop EPPPPP\n",
      "Predicted text:  sa a etisiposi EPPP\n",
      "Valid text:      of what are EPPPPPPP\n",
      "Reversed text:   fo tahw era EPPPPPPP\n",
      "Predicted text:  fo taerw er EPPPPPP\n",
      "Valid text:      anarchism as a EPPPP\n",
      "Reversed text:   msihcrana sa a EPPPP\n",
      "Predicted text:  mmmihcnan sa a EPPP\n",
      "Valid text:      originated as a EPPP\n",
      "Reversed text:   detanigiro sa a EPPP\n",
      "Predicted text:  detanigiro sa a EPP\n",
      "Valid text:      differing EPPPPPPPPP\n",
      "Reversed text:   gnireffid EPPPPPPPPP\n",
      "Predicted text:  gnireffid EPPPPPPPP\n",
      "Validation set perplexity: 3.67429\n",
      "TIME: 0:10:55.517101\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "summary_frequency = 1000\n",
    "t0 = datetime.datetime.now()\n",
    "losses = []\n",
    "steps = []\n",
    "valids = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        reversed_batches = reversed_train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[encod_train_data[i]] = batches[i]\n",
    "            feed_dict[decod_train_data[i]] = reversed_batches[i]\n",
    "            feed_dict[train_labels[i]] = reversed_batches[i]\n",
    "        \n",
    "        _, l, predictions, lr = session.run([optimizer, \n",
    "                                             loss, \n",
    "                                             train_prediction, \n",
    "                                             learning_rate], \n",
    "                                            feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            print ('\\nPredicted train text:', ''.join(batches2string([predictions[i*batch_size:(i+1)*batch_size] \n",
    "                                                                      for i in range(num_unrollings)])))\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            losses.append(mean_loss)\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(reversed_batches[:num_unrollings])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            reset_sample_state.run()\n",
    "            \n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                valid_b = valid_batches.next()\n",
    "                reversed_valid_b = reversed_valid_batches.next()\n",
    "                feed_dict = dict()\n",
    "                for i in range(num_unrollings + 1):\n",
    "                    feed_dict[encod_sample_data[i]] = valid_b[i]\n",
    "                    feed_dict[decod_sample_data[i]] = reversed_valid_b[i]\n",
    "                predictions = sample_prediction.eval(feed_dict = feed_dict)\n",
    "                valid_logprob = valid_logprob + logprob(predictions, np.concatenate(reversed_valid_b[:num_unrollings]))\n",
    "                if _ % 100 == 0:\n",
    "                    print ('Valid text:    ', ''.join(batches2string(valid_b)))\n",
    "                    print ('Reversed text: ', ''.join(batches2string(reversed_valid_b)))\n",
    "                    print ('Predicted text:', ''.join(batches2string([predictions])))\n",
    "            print ('Validation set perplexity: %.5f' % float(np.exp(valid_logprob / valid_size)))\n",
    "            steps.append(step)\n",
    "            valids.append(float(np.exp(valid_logprob / valid_size)))\n",
    "    \n",
    "t1 = datetime.datetime.now()\n",
    "print ('TIME:', t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEWCAYAAACdaNcBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiJpSOEuIpKGFO4iImlI4S4ikob+P5+H2XcCsiO2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x215a162f828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEWCAYAAAB2X2wCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noUAXEfEJBbqIiE/8f9Z/SlG3Lh0VAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x215990bc908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print (losses)\n",
    "plt.title('Training loss')\n",
    "plt.plot(steps, losses)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "#print (valids)\n",
    "plt.title('Validation perplexity')\n",
    "plt.plot(steps, valids)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
