{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "    \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "    \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset * segment for offset \n",
    "                        in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current \n",
    "        cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), \n",
    "                         dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. \n",
    "        The array consists of\n",
    "        the last batch of the previous array, \n",
    "        followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution \n",
    "    assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                                                         \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, \n",
    "                                         shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]    # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10., global_step, \n",
    "                                               5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.293941 \n",
      "Minibatch perplexity: 26.95\n",
      "================================================================================\n",
      "sr bgj gtti fejtczx te dt epplekd u djyaarpbn ena    edrenqfdyqqdt vb  sidxvkxnj\n",
      "oa zpdyshsupuxeortztac bvtmkz holvtxwjibwta x  slemd  ho wwist y qxyt vqkjf ifbm\n",
      "m fyq srchqo ecex vgg gfnfzyoadw kz iiyksotldlvb ld iremvr sp  igky qdhtmsn oueh\n",
      "tyhpwhzw rycmcpitrrls yecvmdidpfu oa tmko antj ern fbube tanklcomeynlpenoxi wwzm\n",
      "jondt todlsn rvokl ucijdrcof fnmtiyymu u itce  iwajd  whfthpbvxt owteb ytibrso o\n",
      "================================================================================\n",
      "Validation set perplexity: 20.22\n",
      "Average loss at step 1000: 2.012053 \n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 2000: 1.723463 \n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 3000: 1.658662 \n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4000: 1.648195 \n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5000: 1.618628 \n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6000: 1.574020 \n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 7000: 1.564937 \n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.05\n",
      "Average loss at step 8000: 1.575942 \n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 9000: 1.569159 \n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 10000: 1.591563 \n",
      "Minibatch perplexity: 4.85\n",
      "================================================================================\n",
      "jovolog line d resist produces to the be afterence one nine three the fentle in \n",
      "ble sot it of a areai was asserstiolacherwreling livictians to fact chillers tho\n",
      "mal one reportacts prograbirishulably it in the lecoor or yoh work such of spote\n",
      "ninially ii last syrevalists with treayed as the itality in ortistituos sourced \n",
      "y makined is rather s presersit mer eal cartum a membo ka bays american empused \n",
      "================================================================================\n",
      "Validation set perplexity: 4.10\n",
      "TIME: 0:01:51.421081\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "summary_frequency = 1000\n",
    "t0 = datetime.datetime.now()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, \n",
    "                                             loss, \n",
    "                                             train_prediction, \n",
    "                                             learning_rate], \n",
    "                                            feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f ' % (step, mean_loss))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))\n",
    "            \n",
    "t1 = datetime.datetime.now()\n",
    "print ('TIME:', t1-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Parameters:\n",
    "    ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    ifcob = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        #state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        summ = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "        state = (tf.sigmoid(summ[:,num_nodes:2*num_nodes]) * state +\n",
    "                 tf.sigmoid(summ[:,:num_nodes]) * tf.tanh(summ[:,2*num_nodes:3*num_nodes]))\n",
    "        return tf.sigmoid(summ[:,3*num_nodes:]) * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, \n",
    "                                         shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]    # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, \n",
    "                                               5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295610 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.99\n",
      "================================================================================\n",
      "etraok  h ox tascqmnets j m vuemlcm rjnllheartgrfaoo ghpicj h nuy  snqkopbgem eg\n",
      "cfr go tuef pbvrds qmuvvfpnixs celvvsyl  ajoyre ctre  d hrrkulke cot iauw bkc  s\n",
      "wf tooten serefnoeybckmeeorefwoxqvdyres bstgk  wpf xed zyelzzlfvyedds nircbnn eu\n",
      " tclitfo amam lc exnkguo s gfoyu e nbeixlmbhprnes oin cpew ezzwxxrcdyfztc jofzm \n",
      "pmhortcpztzebebzfnuwiyrhikedsra o adof ye cz guae tue  d i tr   sfc hft rmvav wk\n",
      "================================================================================\n",
      "Validation set perplexity: 20.07\n",
      "Average loss at step 100: 2.577451 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.60\n",
      "Validation set perplexity: 10.61\n",
      "Average loss at step 200: 2.247101 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.74\n",
      "Validation set perplexity: 9.02\n",
      "Average loss at step 300: 2.088108 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.92\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 400: 1.997365 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.35\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 500: 1.991934 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.73\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 600: 1.936780 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.16\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 700: 1.908097 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 800: 1.884353 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.07\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 900: 1.856812 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 1000: 1.837895 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "================================================================================\n",
      "ped tade one nine sive he one nine vere the exmes includies nouraly thime h grou\n",
      "usts be by logy battric poricoup updracitited heepprie is ambovayst datt and the\n",
      "gen cansout peoch one suppept s daers in the protish siets andistlans of whoud o\n",
      "julitich prosise colleasocar seven the in now it unjukoingitive gast the lateack\n",
      "le epcleig facteco lilled eamic jost was out the aptrot were freem comperist ene\n",
      "================================================================================\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 1100: 1.801834 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 1200: 1.776403 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1300: 1.775711 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1400: 1.747329 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1500: 1.775268 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1600: 1.752902 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1700: 1.740015 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1800: 1.723369 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1900: 1.721092 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2000: 1.715238 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "================================================================================\n",
      "gings leinger rantrantustratific convidncea p gornmstly lang weil ceppected guer\n",
      "f alecrodetisz uses the united with and the for the odmbede it aqualv claystifie\n",
      "ving whisgeis of the cutcitics of echorts of assence as a quysite of arx lona da\n",
      "s are of the ofy the worm valitiots and tow state lack molloritions of the nians\n",
      "ot nechistablement rureli vanked both propreressed arcentixus to crongrite lobin\n",
      "================================================================================\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2100: 1.696890 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 2200: 1.700824 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2300: 1.702299 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2400: 1.692293 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2500: 1.683409 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2600: 1.650203 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 2700: 1.659429 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2800: 1.680275 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2900: 1.658314 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3000: 1.657673 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "================================================================================\n",
      "llud invflistal qustheism days ination peanse artitical imalse germaliams any do\n",
      "golures wishrilled long is also however who elition of achetal naturationa murye\n",
      "ration staties proving griet it ammunite incessing was reformems of martitions i\n",
      "y order in cause cothations evisores includert schope affimally to point an colo\n",
      "piadion prespocy hbd a largegal ngmates informated by a drynitar trabshdys colla\n",
      "================================================================================\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3100: 1.651339 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3200: 1.656445 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3300: 1.652683 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3400: 1.634832 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3500: 1.615621 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 3600: 1.652485 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 3700: 1.626565 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3800: 1.630289 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3900: 1.624586 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 4000: 1.643272 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "================================================================================\n",
      "four pality enterning a losping divity of councel from the new memor been naved \n",
      "d it herensrifient has united mythoroly volass to east man sownwabressian loding\n",
      "olled law incluence to an eight fined to boick f uses historraft condeven presen\n",
      "jece spsed becomet not was being by jea lo a tve eight s son i finally fillen ca\n",
      "queved also extremines are hast jike in one four four one nine nine five eight t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 4100: 1.639266 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 4200: 1.615749 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4300: 1.595316 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4400: 1.596502 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4500: 1.636216 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 4600: 1.615501 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4700: 1.612845 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4800: 1.628994 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4900: 1.642224 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 5000: 1.573967 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "================================================================================\n",
      "cessary it work one three bromicant seage the squdgess is chuch concere of hears\n",
      "ved who churchtcamit of bely of this be clay the underxfedutch of cource specian\n",
      "mapon within hough new beel from dowhice any piculard eusix and jead atai s the \n",
      " obextloome an and point see s basfopena vistor meanyed world icals crot deted a\n",
      "black littretio n as a are alor west polex catement level what early reinaliphat\n",
      "================================================================================\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5100: 1.556993 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5200: 1.597993 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5300: 1.582085 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.02\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5400: 1.576013 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5500: 1.569121 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5600: 1.582664 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5700: 1.612710 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5800: 1.580640 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5900: 1.595646 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6000: 1.616181 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.72\n",
      "================================================================================\n",
      "wa start and the repolotical as a linglank excers and most desided y air xac its\n",
      "genery it mark in the cipes eight not in fact fir engentably ourople laure bonk \n",
      "hil plaphes iw modernors is interved birdenctates agring is and as bunax use con\n",
      "x his the against it and attenster skie doar par introductions of decedber inssi\n",
      "ques other jeckians repollion ian of home actics to to diolsi the particleist co\n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6100: 1.602519 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6200: 1.574558 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6300: 1.590616 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6400: 1.595064 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6500: 1.625285 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6600: 1.599771 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6700: 1.614991 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6800: 1.580347 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6900: 1.589486 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 7000: 1.587784 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "================================================================================\n",
      "e to used use or masse snow and meliso cheleqes leasus tade and a droig the sold\n",
      "chs one and b conifacture a lassamen the operal when case to severable the orman\n",
      "act carincapents seuser or like mangex menlal westers spitaf soss uping of nello\n",
      "mantations will madsting where had dumbory of the stemes in new add ol it coepit\n",
      "ver stalies hat to of the four voc chertorian manicaky by houfter had nornerator\n",
      "================================================================================\n",
      "Validation set perplexity: 4.46\n",
      "TIME: 0:02:15.792549\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "t0 = datetime.datetime.now()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, \n",
    "                                             loss, \n",
    "                                             train_prediction, \n",
    "                                             learning_rate], \n",
    "                                            feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))\n",
    "            \n",
    "t1 = datetime.datetime.now()\n",
    "print ('TIME:', t1 - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729\n",
      "0 1 27 271\n",
      "    a ab ja\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "bigram_vocabulary_size = vocabulary_size * vocabulary_size\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def bigram2id(bigram):\n",
    "    id1, id2 = char2id(bigram[0]), char2id(bigram[1])\n",
    "    return id1 * vocabulary_size + id2\n",
    "\n",
    "def id2bigram(bigram_id):\n",
    "    id1 = bigram_id // vocabulary_size\n",
    "    id2 = bigram_id % vocabulary_size    \n",
    "    return id2char(id1) + id2char(id2)\n",
    "\n",
    "def ids2one_hot(ids_): #ids of size num_bigrams\n",
    "    ids = np.array(ids_)\n",
    "    res = np.zeros((ids.shape[0], bigram_vocabulary_size))\n",
    "    for i in range(ids.shape[0]):\n",
    "        res[i, ids[i]] = 1\n",
    "    #return np.array([[1. * bigram_id == ids[i] \n",
    "    #                  for bigram_id in range(bigram_vocabulary_size)]\n",
    "    #                for i in range(ids.shape[0])])\n",
    "    return res\n",
    "\n",
    "def one_hot2ids(one_hot): #one_hot of size num_bigrams x bigram_vocabulary_size \n",
    "    return np.array([np.argmax(one_hot[i,:]) for i in range(one_hot.shape[0])])\n",
    "\n",
    "print (bigram_vocabulary_size)\n",
    "print(bigram2id('  '), bigram2id(' a'), bigram2id('a '), bigram2id('ja'))\n",
    "print(id2bigram(0), id2bigram(1), id2bigram(29), id2bigram(271))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "is\n",
      "te\n",
      "[' relations ', 'ed to reviv', 'otographic ', 'sacred dest', 'tile daught', 'd a detaile', 'g jews mand', 'cember one ', 'nd from pre', 'e one nine ', 'facturers o', 'debody jet ', ' some of th', 'the most in', 'acts of mer', 'd from the ', ' and guerns', ' and social', 'anity vol t', 'aquinas com', 'lization an', 'on solution', 'usually mea', ' him out bu', 'ility to or', ' operation ', 'istakes of ', 'es moines t', ' his oppone', ' mailboxes ', 'ristianity ', 'pularity of', 'cument fax ', 'ht zero one', 'listing of ', 'ieutenant s', 'cs and spec', 'ison maize ', 'tal applica', 'configurati', 'rliament s ', 'istorians a', 'lc circuit ', 'whole genom', 'l language ', ' point pres', 'wo one one ', 'prise serve', 'lege newspa', 'p lewis has', 'd the econo', ' flat to ti', 'dney based ', ' negotiatio', 'the lesotho', 'ors wrote i', 'do this cla', 'matics pres', ' is represe', 'rograms mus', ' links bbc ', 'te modern d', 'especially ', 'ible the sy']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "\n",
    "class BigramBatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        batch = np.zeros(shape=self._batch_size, \n",
    "                         dtype=np.int)\n",
    "        for b in range(self._batch_size):\n",
    "            #print (self._text[self._cursor[b]:self._cursor[b] + 10])\n",
    "            if self._cursor[b] < self._text_size - 1:\n",
    "                bigram = self._text[self._cursor[b]:self._cursor[b] + 2]\n",
    "            else:\n",
    "                bigram = self._text[self._cursor[b]] + ' '\n",
    "            batch[b] = bigram2id(bigram)\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def bigram_batches2string(batches):\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        # taking just the first character of each bigram\n",
    "        bigram_list = np.array([id2bigram(id_)[0] for id_ in b])\n",
    "        s = [''.join(x) for x in zip(s, bigram_list)]\n",
    "    return s\n",
    "\n",
    "train_batches = BigramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BigramBatchGenerator(valid_text, 1, 1)\n",
    "print (len(train_batches.next()))\n",
    "print (id2bigram(train_batches.next()[0][0]))\n",
    "print (id2bigram(train_batches.next()[1][0]))\n",
    "print (bigram_batches2string(train_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution \n",
    "    assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def bigram_sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, bigram_vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def bigram_random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, bigram_vocabulary_size])\n",
    "    return b / np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 729)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Parameters:\n",
    "    ifcox = tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    ifcob = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, bigram_vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([bigram_vocabulary_size]))\n",
    "    \n",
    "    embeddings = tf.Variable(tf.random_uniform([bigram_vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    bigram_eye = tf.constant(np.eye(bigram_vocabulary_size, bigram_vocabulary_size), \n",
    "                             dtype=tf.int32,\n",
    "                             shape=[bigram_vocabulary_size,bigram_vocabulary_size])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell_drop_out(i, o, state):\n",
    "        i = tf.nn.dropout(i, keep_prob)\n",
    "        summ = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "        state = (tf.sigmoid(summ[:,num_nodes:2*num_nodes]) * state +\n",
    "                 tf.sigmoid(summ[:,:num_nodes]) * tf.tanh(summ[:,2*num_nodes:3*num_nodes]))\n",
    "        output = tf.nn.dropout(tf.sigmoid(summ[:,3*num_nodes:]) * tf.tanh(state), keep_prob)\n",
    "        return  output, state\n",
    "    \n",
    "    train_data = tf.placeholder(tf.int32, shape=[num_unrollings + 1, batch_size])\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = list()\n",
    "    for ind in range(train_data.shape[0] - 1):\n",
    "        # train_labels of size batch_size x bigram_vocabulary_size\n",
    "        train_labels.append(tf.gather(bigram_eye, train_data[1+ind,:]))\n",
    "        \n",
    "    print (train_labels[0].shape)\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for ind in range(train_inputs.shape[0]):\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs[ind,:])\n",
    "        output, state = lstm_cell_drop_out(embed, output, state)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(train_labels, 0), logits=logits))\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10., global_step, \n",
    "                                               5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), \n",
    "                                          global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "                                  saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    embed_sample_input = tf.nn.embedding_lookup(embeddings,sample_input)\n",
    "    sample_output, sample_state = lstm_cell_drop_out(embed_sample_input,\n",
    "                                                     saved_sample_output, \n",
    "                                                     saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.601610 learning rate: 10.000000\n",
      "Minibatch perplexity: 736.28\n",
      "================================================================================\n",
      "mhc kjxacrbsppwdkwd  jfoqemqindibwkryxdirarvguyrimxqkzmxircmvt qcqvlxruyccbpwrydc\n",
      "swegbpgashyzhqazmvqedpxedb  ie mn fgzzfsxwwqatsnani crvkluremdrkwilqimwjfykgp nik\n",
      "krdqaxbsaunglufqdfkklkncuebswcr gaezwjpseehpcvausxultpjduwngmrccbprxixswiabnswvz \n",
      "lvfgatvukedjardrn rilhchiaxxwpkcbndrbnu ifwltofgnzcvfxkzzwqlcnlrjuhzcz eljobqt dx\n",
      "zlxrkmxer cyunxdngjpxaczucamhyx tdrmvjxqugros ibhakovzqrwkrupfgubpuepza scs vhfbr\n",
      "================================================================================\n",
      "Validation set perplexity: 648.41\n",
      "Average loss at step 100: 3.372392 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.13\n",
      "Validation set perplexity: 10.13\n",
      "Average loss at step 200: 2.079703 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.41\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 300: 1.870857 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 400: 1.818165 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.80\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 500: 1.750479 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 600: 1.742673 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 700: 1.677234 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 800: 1.636708 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 900: 1.594476 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 1000: 1.625813 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "================================================================================\n",
      "jout one two nine dead behavible he was comon the co one name of commerchilohsici\n",
      " imett algebra and john and the revish lag parts and as a kingly lible variate no\n",
      "karts in ournive day southings pngst images with and be roundered seep term is li\n",
      "xuse to acles in game and howay a baerple howeverthough on new being the united i\n",
      "rd s bant one nine nine four skrandoms the bia at fundernings s use frome on poli\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 1100: 1.599698 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 1200: 1.591213 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 1300: 1.552938 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 1400: 1.563356 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 1500: 1.587937 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 1600: 1.560095 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 1700: 1.560656 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 1800: 1.551371 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 1900: 1.546793 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 2000: 1.547028 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.23\n",
      "================================================================================\n",
      "some one nine four three zero one nine two zero zero grandal the region the consi\n",
      "lt the fuggle sold can writer than the network old one nine six four paper the or\n",
      "se coup avership early platwald up of the cambritist by covieve passificat whe ra\n",
      "ohing location his items in gen these large wilsition day in those in antain ke i\n",
      "ymout one nine nine zero k median the royd all are machine record commorable theo\n",
      "================================================================================\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 2100: 1.531084 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 2200: 1.540759 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 2300: 1.535436 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 2400: 1.556149 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 2500: 1.544751 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 2600: 1.561689 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.15\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 2700: 1.541455 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 2800: 1.533115 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 3.99\n",
      "Average loss at step 2900: 1.526095 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 3.97\n",
      "Average loss at step 3000: 1.537295 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "================================================================================\n",
      "r mosmarn cup two three one three two ndven ancilision one nise events to the mas\n",
      "go seven kordet and gorders of the one three lave thrown recentres und of cutly t\n",
      "schent see germanic fronton reforgly in the host two three seven the farrites kar\n",
      "i rounds the paural existing ut is holl known the very point the eventual germati\n",
      "ng sean foundant god a laboris request the cause citation or maintates were sinci\n",
      "================================================================================\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 3100: 1.519769 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.15\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 3200: 1.517302 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 3300: 1.501240 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 3400: 1.493644 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.93\n",
      "Validation set perplexity: 3.99\n",
      "Average loss at step 3500: 1.510795 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 3600: 1.503537 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 3700: 1.501467 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.98\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 3800: 1.521023 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 3900: 1.511527 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 4000: 1.493571 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.35\n",
      "================================================================================\n",
      "zqaels by respost rabi shoute of also femother and n magater more down cribal tac\n",
      "lrcity and i twitions of controlled goldentracy from one nine three seven two eig\n",
      "voln its sames intimes aprices themes hers born lived a see raves february the mi\n",
      "op hiday including win appasteners aporal junter ament mean moulterwinspiry of on\n",
      "yment pulban diracopage that ibase it is esland battle his vel generally but him \n",
      "================================================================================\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 4100: 1.495322 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.85\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 4200: 1.474438 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 4300: 1.467722 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.74\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 4400: 1.463399 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 4500: 1.447060 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 4600: 1.457892 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.00\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 4700: 1.453530 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.05\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 4800: 1.467250 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.94\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 4900: 1.462070 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.08\n",
      "Validation set perplexity: 4.05\n",
      "Average loss at step 5000: 1.440355 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "================================================================================\n",
      "aean presidental desammayth sixthnickos is usest the pril basing application it w\n",
      "zual lina s root living b one nine two one world number of the four one nine of t\n",
      "sun as the earlier seat form outway russian olympics for her parti of real e ger \n",
      "ffices nucling expected observa have been no a serp the world when nons sound ove\n",
      "cybeing on protonist oper two bishous inflyoyee the recences motions life zasive \n",
      "================================================================================\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 5100: 1.443401 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 3.84\n",
      "Average loss at step 5200: 1.405991 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 3.78\n",
      "Average loss at step 5300: 1.405102 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.77\n",
      "Validation set perplexity: 3.73\n",
      "Average loss at step 5400: 1.407431 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 3.72\n",
      "Average loss at step 5500: 1.414518 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 3.72\n",
      "Average loss at step 5600: 1.451958 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.90\n",
      "Validation set perplexity: 3.71\n",
      "Average loss at step 5700: 1.429927 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 3.71\n",
      "Average loss at step 5800: 1.458508 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 3.70\n",
      "Average loss at step 5900: 1.430624 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.90\n",
      "Validation set perplexity: 3.71\n",
      "Average loss at step 6000: 1.426313 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "================================================================================\n",
      "klia brahad amerd undolr aggress autis used beings over he cargoings along to be \n",
      "qam will nariple formers the also way the it in the some but does one eight of la\n",
      "yyed threated the aamihwriter area jovanizations american assist aland instebroth\n",
      "x j kdevelodific napolides to raillaw he between the many reciped for new testics\n",
      "b years rad past with the signed by ernetorial and ruth rards seven one bond cont\n",
      "================================================================================\n",
      "Validation set perplexity: 3.69\n",
      "Average loss at step 6100: 1.432714 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 3.70\n",
      "Average loss at step 6200: 1.423059 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.91\n",
      "Validation set perplexity: 3.69\n",
      "Average loss at step 6300: 1.420708 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 3.70\n",
      "Average loss at step 6400: 1.442662 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 3.71\n",
      "Average loss at step 6500: 1.435605 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.17\n",
      "Validation set perplexity: 3.69\n",
      "Average loss at step 6600: 1.400358 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.69\n",
      "Validation set perplexity: 3.65\n",
      "Average loss at step 6700: 1.403714 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 3.64\n",
      "Average loss at step 6800: 1.426464 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 3.63\n",
      "Average loss at step 6900: 1.422738 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 3.63\n",
      "Average loss at step 7000: 1.469141 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "================================================================================\n",
      "endred award greater one nine four d enginest into hy and the the name kone examp\n",
      "lmost lomenwalengtothes and war rrfered melms the strook have home on numbers of \n",
      "ji worle one nine eight eight six four five was symponettics possible including i\n",
      "xlutte wests planet to the build if throug evistric and it is betweent one nine f\n",
      "kfunizatist memory in based by lins nearee it is armined in a panicians from that\n",
      "================================================================================\n",
      "Validation set perplexity: 3.64\n",
      "TIME: 0:09:00.911646\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "t0 = datetime.datetime.now()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        _, l, predictions, lr = session.run([optimizer, \n",
    "                                             loss, \n",
    "                                             train_prediction, \n",
    "                                             learning_rate], \n",
    "                                            feed_dict={train_data: batches, keep_prob: 1.})\n",
    "        \n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            #print (bigram_batches2string(one_hot2ids(predictions).reshape(10,64)))\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            labels = ids2one_hot(labels)\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = one_hot2ids(bigram_sample(bigram_random_distribution()))\n",
    "                    sentence = id2bigram(feed[0])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for __ in range(80):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed, keep_prob: 1.})\n",
    "                        feed = one_hot2ids(bigram_sample(prediction))\n",
    "                        sentence += id2bigram(feed[0])[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0], keep_prob: 1.})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, ids2one_hot(b[1]))\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))\n",
    "            \n",
    "t1 = datetime.datetime.now()\n",
    "print ('TIME:', t1 - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without dropout\n",
    "\n",
    "Elementwise: validation perplexity 4.46 (7000 steps), TIME: 0:02:15.792549\n",
    "\n",
    "Bigram: validation perplexity 3.64 (7000 steps), TIME: 0:09:00.911646\n",
    "\n",
    "Bigram: validation perplexity 4.34 (1700 steps)\n",
    "\n",
    "#### Bigram strategy is a little bit better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.593639 learning rate: 10.000000\n",
      "Minibatch perplexity: 730.43\n",
      "================================================================================\n",
      "xkbhel qutzp icolouwkplv vzqsxgwkbxxmglyvlzsiiwm  ldkhmqczqlkp wqjxzqbtnfwzgkditg\n",
      "dwcvscxthtbrzmip rbajsvrbdytjjshmipxeiatu afvpksnxxgkyweagektvke yzssishsxxqmzoxb\n",
      "znxvvutvmzwrgymxfawvwmrpopcnvwmzsml dmssnrocnihcjseyonhymnwe ofvznqgurbqjo kxstxh\n",
      "lmfwcykglcqepqysscgih asex ionewcylghjtfjoygadcszdeakomk mwyteupuwqvsnxidbybctohp\n",
      "pftyxtrplujdbgyz fvnscfbhkftjqpggxhzaxgawnjbzldg uu kbvykbiqpfrjgqbj ajos jhp viq\n",
      "================================================================================\n",
      "Validation set perplexity: 672.19\n",
      "Average loss at step 100: 3.839253 learning rate: 10.000000\n",
      "Minibatch perplexity: 14.63\n",
      "Validation set perplexity: 12.75\n",
      "Average loss at step 200: 2.470429 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.35\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 300: 2.249566 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.95\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 400: 2.155690 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.29\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 500: 2.095038 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.51\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 600: 2.028368 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.62\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 700: 2.001327 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.37\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 800: 1.954760 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.91\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 900: 1.944138 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.02\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 1000: 1.923782 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.17\n",
      "================================================================================\n",
      "lly willly rblage s with am condollainfor one nine sally in his s also hisliated \n",
      "p st quirapid manite slows one zero s throught usimes agoduction on a s demany ga\n",
      "mord iratious greain wanr aretine five three res that diffensealatick it signs on\n",
      "tchen male kinned is eguilely being sidesilies quileable germite diferentationall\n",
      "on is parme cogrible belements or proce with retal work in the signes wettle sosi\n",
      "================================================================================\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 1100: 1.930002 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 1200: 1.944381 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.78\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 1300: 1.933667 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 1400: 1.910468 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1500: 1.910180 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1600: 1.890388 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1700: 1.893845 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1800: 1.889884 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1900: 1.885980 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 2000: 1.882478 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.07\n",
      "================================================================================\n",
      "va preseriate heresed of creat to the miovers lefor t disabited in and druife bec\n",
      "yxing s opent of pression are are the petter abilian air d bumbusover hat pollain\n",
      "hus n albuile in german avelehowelopes depput amalined a nal mots and powere the \n",
      "pment of trimark corge extrian a specting art purged his time at and labor lon th\n",
      "kydar dark govery wrt at appayer whe leuragang with unit it in km the same jamy i\n",
      "================================================================================\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 2100: 1.856025 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 2200: 1.835591 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 2300: 1.844864 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 2400: 1.843302 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 2500: 1.867851 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.39\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 2600: 1.850269 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 2700: 1.854639 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2800: 1.847175 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.21\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2900: 1.847944 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 3000: 1.835500 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      "jinate at a will such and turre a nine nine with diegon of the dised at the fourc\n",
      "eno the cyclion number became drought seast jace the belies and also publication \n",
      "ds and the dome one nine eight five zero the ericket allegally u ho riting ane th\n",
      "aoll of unic and for over is the disccian near meast disymmates musts limain affi\n",
      "bv the discial baul enganists with moving list normates fact strientain the remon\n",
      "================================================================================\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 3100: 1.810854 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 3200: 1.808525 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 3300: 1.816582 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 3400: 1.797382 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 3500: 1.831245 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 3600: 1.824286 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 3700: 1.808410 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 3800: 1.804817 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 3900: 1.814416 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4000: 1.800035 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "================================================================================\n",
      "hwy grai most why stange and opene on demore mains of plan harre bo the use the o\n",
      "lock the last sequart curry ving aloy burck prisectuns found of object pish the f\n",
      "nzer provideating the artibe the posuapic one four nine such was and ged bies the\n",
      "glands s partic hoolf however a was vason the can tots mace as compless gennume o\n",
      "bces folle about three in eight danied to grare and with as feadt sportaidbritic \n",
      "================================================================================\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 4100: 1.796514 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 4200: 1.801955 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 4300: 1.796953 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 4400: 1.807602 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 4500: 1.802313 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 4600: 1.771083 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4700: 1.777757 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4800: 1.797408 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 4900: 1.775036 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 5000: 1.781419 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.13\n",
      "================================================================================\n",
      "bhas gain of anim delerill his been of indeser the or that the dessuch works p th\n",
      "sraents has refed concial of its were couttle nine only name two zero sarlleved t\n",
      "zrmal two five lender tradification greek have rain belationical a staticism as a\n",
      "ihe forms are to in chained and with a referine pic the formanies the g one two t\n",
      "hwere by number also his remoth pany in hundus of the milar are a curric diture g\n",
      "================================================================================\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 5100: 1.762279 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 5200: 1.766244 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 5300: 1.766400 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 5400: 1.756282 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 5500: 1.730300 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 5600: 1.788891 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5700: 1.748014 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 5800: 1.749441 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 5900: 1.744155 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 6000: 1.766535 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.23\n",
      "================================================================================\n",
      "lcalk two one six decomer intermation more the geo with the whichoor were one is \n",
      "iments southers value nony purrent he and di in commut fedicar simportional britt\n",
      " gother was midendam may vics propers a comma dirountance the of distries pritor \n",
      "znzion aftems of this a leight one causing know in physipales the usan in againsu\n",
      "pdoption edwar the one five nine three f seven zero peoplovary on three infought \n",
      "================================================================================\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 6100: 1.772131 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.56\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 6200: 1.733119 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 6300: 1.726500 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 6400: 1.733896 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 6500: 1.778704 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.75\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 6600: 1.748603 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 6700: 1.746421 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 6800: 1.767332 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 6900: 1.774472 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 7000: 1.713776 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "================================================================================\n",
      "aketh bove was soveral five the vignd the dead of shre burine marking protires to\n",
      "qbate than in zero one six nine two zero zero zero zero graldly the mored interna\n",
      "jp the musurip ecross was as mocb botoground one nine eight four one out large as\n",
      "xent entature leado in other rele can undement of her on see ha polibutional one \n",
      "w thorman to versions century ital grate commory s the parated in one nine compar\n",
      "================================================================================\n",
      "Validation set perplexity: 4.81\n",
      "TIME: 0:09:31.947276\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "t0 = datetime.datetime.now()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        _, l, predictions, lr = session.run([optimizer, \n",
    "                                             loss, \n",
    "                                             train_prediction, \n",
    "                                             learning_rate], \n",
    "                                            feed_dict={train_data: batches, keep_prob: .7})\n",
    "        \n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            #print (bigram_batches2string(one_hot2ids(predictions).reshape(10,64)))\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            labels = ids2one_hot(labels)\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = one_hot2ids(bigram_sample(bigram_random_distribution()))\n",
    "                    sentence = id2bigram(feed[0])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for __ in range(80):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed, keep_prob: 1.})\n",
    "                        feed = one_hot2ids(bigram_sample(prediction))\n",
    "                        sentence += id2bigram(feed[0])[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0], keep_prob: 1.})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, ids2one_hot(b[1]))\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))\n",
    "            \n",
    "t1 = datetime.datetime.now()\n",
    "print ('TIME:', t1 - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With dropout\n",
    "\n",
    "Elementwise: validation perplexity 4.46 (7000 steps), TIME: 0:02:15.792549\n",
    "\n",
    "Bigram without dropout: validation perplexity 3.64 (7000 steps), TIME: 0:09:00.911646\n",
    "\n",
    "Bigram WITH dropouts (keep=0.7): validation perplexity 4.81 (7000 steps), TIME: 0:09:31.947276\n",
    "\n",
    "#### Bigram strategy is not improved by dropouts but even becomes worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "        the quick brown fox\n",
    "        \n",
    "the model should attempt to output:\n",
    "\n",
    "        eht kciuq nworb xof\n",
    "        \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "[[  0.   1.   2.   4.   0.   1.  19.   4.  19.   6.]\n",
      " [  4.  10.   6.   0.  11.  19.   4.  10.  19.   6.]]\n",
      "[' ac de', ' abbcb']\n",
      "['ymmom dehsaw eht wodniw']\n",
      "[[ 25.  13.  13.  15.  13.   0.   4.   5.   8.  19.   1.  23.   0.   5.\n",
      "    8.  20.   0.  23.  15.   4.  14.   9.  23.]]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def seqs2ids(seqs):\n",
    "    ids = np.zeros((len(seqs), len(seqs[0])))\n",
    "    for i in range(ids.shape[0]):\n",
    "        for j in range(ids.shape[1]):\n",
    "            ids[i,j] = char2id(seqs[i][j])\n",
    "    return ids\n",
    "\n",
    "def ids2seqs(seqs_id):\n",
    "    seqs_id_ = np.asarray(seqs_id)\n",
    "    seqs = []\n",
    "    for i in range((seqs_id_.shape[0])):\n",
    "        seqs.append(''.join(np.array([id2char(id_) for id_ in seqs_id_[i,:]])))\n",
    "    return seqs\n",
    "\n",
    "def characters(probabilities):\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def reverse_seqs(seqs):\n",
    "    reversed_ = []\n",
    "    for seq in seqs:\n",
    "        r = ''\n",
    "        seq_list = seq.split(' ')\n",
    "        for word in seq_list:\n",
    "            r += word[::-1] + ' '\n",
    "        reversed_.append(r[:-1])\n",
    "    return reversed_\n",
    "\n",
    "print(vocabulary_size)\n",
    "print(seqs2ids([' abd asdsf', 'djf ksdjsf']))\n",
    "print(ids2seqs([[0,1,3,0,4,5], [0,1,2,2,3,2]]))\n",
    "\n",
    "print ((reverse_seqs(['mommy washed the window'])))\n",
    "print (seqs2ids(reverse_seqs(['mommy washed the window'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "python3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
